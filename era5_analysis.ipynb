{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter with large datasets\n",
    "\n",
    "* [Reading Data](#Reading-data)\n",
    "* [Dask](#Dask)\n",
    "* [Processing 60 GB](#Processing-60-GB)\n",
    "* [Saving Data](#Saving-Data)\n",
    "* [Processing 3 TB](#Processing-3-TB)\n",
    "* [Parallel Processing](#Parallel-Processing)\n",
    "\n",
    "## Our objective\n",
    "\n",
    "The target today is to calculate a daily wind magnitude field over Tasmania from ERA5 data\n",
    "\n",
    "## Start a notebook\n",
    "\n",
    "https://github.com/coecms/nci_scripts\n",
    "\n",
    "* `gadi_jupyter` submits a job to the queue, and costs NCI resources. Try to stick to 4 or fewer cpus\n",
    "* `vdi_jupyter.py` connects to the VDI desktop, free but can get congested\n",
    "\n",
    "Both can access storage on /g/data\n",
    "\n",
    "`gadi_jupyter` prints out some help on how to start up a Dask cluster when the notebook starts, we'll get back to that later.\n",
    "\n",
    "To start with we need to find the data. There's a whole bunch of different datasets pre-downloaded at NCI, we want to use these to avoid filling up disk space unneccessarily. You can find information on these datasets on the CMS wiki http://climate-cms.wikis.unsw.edu.au (search google for 'climate cms wiki' if you don't remember the link)\n",
    "\n",
    "## Reading Data\n",
    "\n",
    "We'll be using the surface u and v wind fields, which the wiki says are under `/g/data/ub4/era5/netcdf/surface`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /g/data/ub4/era5/netcdf/surface/10U/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are netcdf format files, they include both arrays with the field's data as well as metadata about coordinates and where the data is from. There's a number of ways to read netcdf data, my favourite is xarray\n",
    "\n",
    "Xarray lets you open multiple files at once to create a time series, using the function xarray.open_mfdataset. There are two ways to do this, specified with the 'combine' parameter. It can concatenate files in the order that their names are in, or it can open up the file and look at the coordinate values to sort the files.\n",
    "\n",
    "For the most part published datasets will be well structured, with their files named using ISO timestamps. This means we can use the quicker 'nested' combining to merge the files together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "\n",
    "ds = xarray.open_dataset('/g/data/ub4/era5/netcdf/surface/10U/2019/10U_era5_global_20190101_20190131.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.open_mfdataset('/g/data/ub4/era5/netcdf/surface/10U/2019/10U_era5_global_*.nc',\n",
    "                           combine='nested', concat_dim='time')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the second version the time axis covers the whole year.\n",
    "\n",
    "ERA5 is a big dataset - let's check how big this field is now that we've loaded it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.u10.nbytes / 1024 ** 3 # Convert bytes to GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is larger than the available memory on VDI - 32 GB, so how is this possible?\n",
    "\n",
    "Data in a Netcdf file is 'lazily loaded' - it only actually gets read when we read the values, either by printing, plotting or saving them to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.u10.sel(latitude=147.3272, longitude=-42.8821, method='nearest').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With xarray you can call `.load()` on a dataset to actually try and load the whole thing - most of the time you don't want to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.u10.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "In addition to the files themselves being lazy there is a second layer at work as well, Dask. If we look at the data inside the `u10` variable we can see that it's made up of multiple 'chunks' - 12 in fact, one for each month's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.u10.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask arrays work just like numpy arrays, however rather than storing values directly, they store how to calculate the array's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array\n",
    "\n",
    "a = dask.array.zeros((10,10), chunks=(5,5))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the array `a` is broken up into four 5x5 chunks, and each chunk is created using the 'zeros' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking the data into chunks helps for large datasets, as you only need to work on one chunk at a time, and can caluclate multiple chunks in parallel.\n",
    "\n",
    "Otherwise you can do pretty much everything you can do with a Numpy array using a Dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dask.array.random.random((10,10), chunks=(5,5))\n",
    "b = dask.array.random.random((10,10), chunks=(5,5))\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work on a Dask array it will build up a graph of operations required to create the output, but it won't actually run any calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a + b).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only need part of an array Dask will only use that part of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a + b)[1,6].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert a Dask array to a Numpy array using the `.compute()` function (or `.load()` on a Xarray DataArray). Only the neccessary parts of the graph will be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a + b)[:,6].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex operations will create a more complex graph, here's a matrix multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.matmul(a, b).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With large datasets there's a tradeoff between chunk size (bigger chunks > more memory used) and graph size (smaller chunks > more complex graph is slower to process)\n",
    "\n",
    "## Processing 60 GB\n",
    "\n",
    "It's not just Dask that breaks up arrays into chunks - this can be done within a NetCDF file as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.u10.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea for Dask chunks to be a multiple of the NetCDF chunk size, but do experiment to see what works best for your use case.\n",
    "\n",
    "You can specify Dask chunks when opening a NetCDF file using the `chunks` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.open_mfdataset('/g/data/ub4/era5/netcdf/surface/10U/2019/10U_era5_global_*.nc',\n",
    "                           combine='nested', concat_dim='time', chunks={'longitude':93*2,'latitude':91*2})\n",
    "ds.u10.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with plain Dask arrays you can do normal Numpy-style operations on files you open with Xarray, and it will build up a graph of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ds.u10\n",
    "\n",
    "ds_v = xarray.open_mfdataset('/g/data/ub4/era5/netcdf/surface/10V/2019/10V_era5_global_*.nc',\n",
    "                           combine='nested', concat_dim='time', chunks={'longitude':93*2,'latitude':91*2})\n",
    "v = ds_v.v10\n",
    "\n",
    "wind = numpy.sqrt(u**2 + v**2)\n",
    "wind.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ERA5 data is hourly, we want daily data for our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the hourly ERA data to daily we can do a resample, getting the mean of each day's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_daily = wind.resample(time='D').mean()\n",
    "wind_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a lot of chunks though - one for each day, so it doesn't work well on large datasets. Try to avoid big jumps in the number of tasks or chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_daily.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'climtas' library I've been developing has some routines to improve chunking performance for resampling and grouping by day of year. They're less flexible than the standard Xarray functions, but handy to have for large datasets as they keep the original chunking\n",
    "\n",
    "https://climtas.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import climtas.blocked\n",
    "\n",
    "wind_daily = climtas.blocked.blocked_resample(wind, time=24).mean()\n",
    "wind_daily.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "\n",
    "Before saving to file check how big the data is - remember disk quota is a resource shared between all members of a project, so don't fill up space you don't need to.\n",
    "\n",
    "It's also a good idea to make a quick plot to make sure you have the right area and the values are reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines('10m')\n",
    "\n",
    "wind_daily.sel(longitude=slice(144,149), latitude=slice(-40, -44)).isel(time=0).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving your data try as much as possible to use compression, it can save a lot of space. This can be set up using the `encoding` parameter of `.to_netcdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.diagnostics\n",
    "\n",
    "wind_daily.name = 'wind'\n",
    "\n",
    "encoding = {\n",
    "    'wind': { # Variable name\n",
    "        'zlib': True, # Turn on compression\n",
    "        'shuffle': True, # Turn on shuffle filter\n",
    "        'complevel': 4, # Compression amount (0-9), 4 is a good choice\n",
    "    }\n",
    "}\n",
    "\n",
    "# Show a progress bar - doesn't work with 'distributed' unfortunately\n",
    "with dask.diagnostics.ProgressBar():\n",
    "    (wind_daily\n",
    "     .sel(longitude=slice(144,149), latitude=slice(-40, -44))\n",
    "     .to_netcdf('/g/data/w35/saw562/analysis_example.nc', encoding=encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the chunking and compression of a file using the `-s` option of `ncdump`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ncdump -hs /g/data/w35/saw562/analysis_example.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing 3 TB\n",
    "\n",
    "The same process works on the entire ERA5 timeseries. The full timeseries of a 2d file is 1.5 TB though, so it might take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_u = xarray.open_mfdataset('/g/data/ub4/era5/netcdf/surface/10U/*/10U_era5_global_*.nc',\n",
    "                           combine='nested', concat_dim='time', chunks={'longitude':93*2,'latitude':91*2})\n",
    "\n",
    "ds_v = xarray.open_mfdataset('/g/data/ub4/era5/netcdf/surface/10V/*/10V_era5_global_*.nc',\n",
    "                           combine='nested', concat_dim='time', chunks={'longitude':93*2,'latitude':91*2})\n",
    "\n",
    "ds_u.u10.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = numpy.sqrt(ds_u.u10**2 + ds_v.v10**2)\n",
    "\n",
    "wind_daily = climtas.blocked.blocked_resample(wind, time=24).mean()\n",
    "wind_daily.name = 'wind'\n",
    "\n",
    "wind_daily.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing a lot of data climtas' throttled save function can be helpful, it limits how much data Dask will read at once so there's less chance of running out of memory during the processing. It also automatically sets up NetCDF compression and chunking.\n",
    "\n",
    "Before doing a calculation like this, consider if you need the whole dataset or just a subset, and get in touch with us at cws_help@nci.org.au to see if the output can be stored centrally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import climtas.io\n",
    "\n",
    "# climtas.io.to_netcdf_throttled(wind_daily, '/g/data/w35/saw562/analysis_example.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing\n",
    "\n",
    "To speed this up a bit we can try running in parallel on Gadi.\n",
    "\n",
    "There is a limit to how parallel you can make this - writing data to the file can't happen in parallel, each process must wait for its turn. Also with a lot of processes Dask will spend time shuffling data between the processes.\n",
    "\n",
    "Use the code that `gadi_jupyter` prints out to start a parallel Dask cluster with the number of CPUs requested by your job - it can't use more than one node's worth of CPUs (so keep the number of cpus under 48)\n",
    "\n",
    "It's important to set the `memory_limit` and `local_directory` options, so you don't run out of memory and don't fill up your home directory\n",
    "\n",
    "```python\n",
    "import os\n",
    "import dask.distributed\n",
    "\n",
    "# Edit as desired\n",
    "threads_per_worker = 1\n",
    "\n",
    "try:\n",
    "    c # Already running\n",
    "except NameError:\n",
    "    c = dask.distributed.Client(\n",
    "        n_workers=int(os.environ['PBS_NCPUS'])//threads_per_worker,\n",
    "        threads_per_worker=threads_per_worker,\n",
    "        memory_limit=f'{3.9*threads_per_worker}gb',\n",
    "        local_directory=os.path.join(os.environ['PBS_JOBFS'],\n",
    "                                     'dask-worker-space')\n",
    "    )\n",
    "c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import numpy\n",
    "import climtas\n",
    "\n",
    "ds_u = xarray.open_mfdataset('/g/data/ub4/era5/netcdf/surface/10U/*/10U_era5_global_*.nc',\n",
    "                           combine='nested', concat_dim='time', chunks={'longitude':93*2,'latitude':91*2})\n",
    "\n",
    "ds_v = xarray.open_mfdataset('/g/data/ub4/era5/netcdf/surface/10V/*/10V_era5_global_*.nc',\n",
    "                           combine='nested', concat_dim='time', chunks={'longitude':93*2,'latitude':91*2})\n",
    "\n",
    "wind = numpy.sqrt(ds_u.u10**2 + ds_v.v10**2)\n",
    "\n",
    "wind_daily = climtas.blocked.blocked_resample(wind, time=24).mean()\n",
    "wind_daily.name = 'wind'\n",
    "\n",
    "#for year, data in wind_daily.groupby('time.year'):\n",
    "#    climtas.io.to_netcdf_throttled(data, f'/g/data/w35/saw562/analysis_example_{year}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Climtas also has an optimised function for generating climatologies that might be handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climtas.blocked.blocked_groupby(wind_daily.sel(time=slice('1980','2019')), time='monthday').percentile(q=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climtas.blocked.blocked_groupby(wind_daily.sel(time=slice('1980','2019')), time='dayofyear').percentile(q=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-20.01]",
   "language": "python",
   "name": "conda-env-analysis3-20.01-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
