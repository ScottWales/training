{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 B: Adding more dimensions, without loops\n",
    "\n",
    "We'll continue on with our heatwave example, this time expanding the analysis to the full 3d dataset\n",
    "\n",
    "We won't be using any explicit loops, instead we'll rely on Dask to automatically order array operations for us\n",
    "\n",
    "To start off with we'll load some libraries and the Dask distributed client, so that the analysis is run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import xarray\n",
    "import dask.array\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "#Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dimensions\n",
    "\n",
    "For the most part working with multi-dimensional data is exactly the same as 1d data. 'Split' functions take the name of the dimension to work on in either case, so operations like calculating the 15 day rolling average work exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip_tasmax  = '/g/data/rr3/publications/CMIP5/output1/CSIRO-BOM/ACCESS1-3/historical/day/atmos/day/r1i1p1/latest/tasmax/tasmax*.nc'\n",
    "\n",
    "ds = xarray.open_mfdataset(cmip_tasmax, chunks={'lat': 50, 'lon': 50, 'time': 1000})\n",
    "tasmax = ds.tasmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasmax_15day = tasmax.rolling(time=15, center=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting - avoid it\n",
    "\n",
    "There are some types of operations that you really need to avoid when working with large datasets.\n",
    "\n",
    "Calculating things like min/max, mean and standard deviation are fine, because they can be calculated in a 'rolling' manner - there's no need to load the entire dataset to do the calculation.\n",
    "\n",
    "Calculating the percentile however requires sorting the data, which is one of the worst things you can do with a large dataset. Sorting requires loading the entire dataset into memory.\n",
    "\n",
    "In this case we could make the assumption that the temperatures are normally distributed, and use that distribution to get the percentiles. This might not always be valid, so you should validate for yourself assumptions like this work\n",
    "\n",
    "**Make an estimate of the 90th percentile at each day of the year at each grid point, assuming temperatures are normally distributed**\n",
    "\n",
    "[`scipy.stats.norm.ppf(q, loc=mean, scale=std)`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) could be useful\n",
    "\n",
    "<a href=\"#ans1\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans1\">\n",
    "<pre><code>\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Show a fancy progress bar\n",
    "with ProgressBar():\n",
    "    tasmax_mean = tasmax_15day.groupby('time.dayofyear').mean('time')\n",
    "    tasmax_std = tasmax_15day.groupby('time.dayofyear').std('time')\n",
    "    \n",
    "    # The ppf function isn't dask aware, so it will load the mean and stddev data\n",
    "    threshold = xarray.DataArray(norm.ppf(0.9, loc=tasmax_mean, scale=tasmax_std), coords=tasmax_mean.coords)\n",
    "</code></pre>\n",
    "</div>\n",
    "\n",
    "**How does our estimate of the 90th percentile compare with the actual value (look at a single grid point)**\n",
    "\n",
    "<a href=\"#ans2\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans2\">\n",
    "<pre><code>\n",
    "with ProgressBar():\n",
    "    threshold.sel(lat=-37.8136, lon=144.9631, method='nearest').plot()\n",
    "    tasmax_15day.sel(lat=-37.8136, lon=144.9631, method='nearest').load().groupby('time.dayofyear').reduce(numpy.nanpercentile, q=90, dim='time').plot()\n",
    "</code></pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy functions\n",
    "\n",
    "To get our filter function to work nicely with dask we have to make a couple changes - when we directly run numpy functions, like `numpy.logical_and()`, these need to be replaced with the dask version, `dask.array.logical_and()` in this case.\n",
    "\n",
    "You'll also need to add an `allow_lazy=True` flag when you call `.reduce()`, to let Xarray know that the filter is a dask-aware function.\n",
    "\n",
    "\n",
    "### Remember - if we've done things right, operations on dask arrays should return immediately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatwave_start_filter(x, axis):\n",
    "    \"\"\"\n",
    "    Returns 1 if a heatwave starts at this time, otherwise nan\n",
    "    \n",
    "    Matches the pattern [*, <0, >=0, >=0, >=0] on the rolling dimension\n",
    "    \n",
    "    Should be called with x.rolling(time=5, center=True).reduce(heatwave_start_filter)\n",
    "    \"\"\"\n",
    "\n",
    "    assert axis % x.ndim == x.ndim - 1\n",
    "    assert x.shape[axis] == 5\n",
    "\n",
    "    left  = dask.array.isnan(x[..., 1]) # Time before this one\n",
    "\n",
    "    right = dask.array.isfinite(x[..., 2:]).all(axis=axis) # This time and two after\n",
    "    \n",
    "    test  = dask.array.logical_and(left, right)\n",
    "    return dask.array.where(test, 1.0, numpy.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    hw_starts = candidates.rolling(time=5, center=True, min_periods=1).reduce(heatwave_start_filter, allow_lazy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation tips\n",
    "\n",
    " * Narrow down your region with `.isel()` before you use a split function\n",
    " * After setting up climatologies `.load()` or save to file so they are pre-calculated\n",
    " * Use smaller chunk sizes to reduce what dask loads\n",
    " \n",
    "http://xarray.pydata.org/en/stable/dask.html#optimization-tips\n",
    "\n",
    "For instance, put the analysis into a function, then use a selected region as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "with ProgressBar():\n",
    "    ax = plt.axes(projection=ccrs.Orthographic(central_longitude=140))\n",
    "    hw_starts.sel(time='1998').count(dim='time').plot(ax=ax, transform=ccrs.PlateCarree())\n",
    "    ax.coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hw_starts(t):\n",
    "    candidates = t.where(t.groupby('time.dayofyear') > threshold)\n",
    "    return candidates.rolling(time=5, center=True, min_periods=1).reduce(heatwave_start_filter, allow_lazy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    ax = plt.axes(projection=ccrs.Orthographic(central_longitude=140))\n",
    "    find_hw_starts(tasmax.sel(time=slice('1997','1999'))).count(dim='time').plot(ax=ax, transform=ccrs.PlateCarree())\n",
    "    ax.coastlines()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-18.04]",
   "language": "python",
   "name": "conda-env-analysis3-18.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
